{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ‡ Welcome to the `loader` module!\n",
    "\n",
    "This is where your urban data journey begins. Whether youâ€™ve got `CSV`, `Parquet`, or `Shapefiles`, weâ€™ll get them loaded up and ready to explore. UrbanMapper provides two main ways to load data:\n",
    "\n",
    "1. **Manual Loading of Local Datasets**: You can load datasets available locally in various formats like CSV, Parquet, and Shapefiles. This is the default approach for working with your own data.\n",
    "2. **Integration with Hugging Face Dataset Library**: UrbanMapper also supports loading datasets from the Hugging Face library via the `from_dataframe()` method. This broadens the possibilities for integrating external data sources seamlessly.\n",
    "\n",
    "**Data source used**:\n",
    "- PLUTO data from NYC Open Data. https://www.nyc.gov/content/planning/pages/resources/datasets/mappluto-pluto-change\n",
    "- Taxi data from NYC Open Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "- **The OSCUR Hugging Face Dataset Source:**\n",
    "The [OSCUR Hugging Face organization](https://huggingface.co/oscur)\n",
    " hosts all datasets associated with [OSCUR](https://oscur.org/): Open-Source Cyberinfrastructure for Urban Computing, a research initiative focused on enabling reproducible, scalable, and accessible data-driven analysis for urban environments.\n",
    "By using the OSCUR datasets, you can skip downloading datasets from Google Drive or official links locally. These datasets are ready to use in all subsequent notebook examples without issue, making your workflow more efficient and seamless.\n",
    "\n",
    "**What youâ€™ll learn**:\n",
    "- How to kick off UrbanMapper.\n",
    "- Loading data from CSV, Parquet, and Shapefile formats.\n",
    "- Loading datasets from Hugging Face with UrbanMapper.\n",
    "\n",
    "Ready? Letâ€™s dive in! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_mapper as um\n",
    "\n",
    "# Start up UrbanMapper\n",
    "mapper = um.UrbanMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "First up, letâ€™s load a CSV file with PLUTO data. Weâ€™ll tell UrbanMapper where to find the longitude and latitude columns so it knows whatâ€™s what and can make sure those colums are well formatted prior any analysis.\n",
    "\n",
    "Note that below we employ a given csv, but you can put your own path, try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"../data/[NYC][USA] MapPluto/CSV/pluto.csv\") # To update with your own path\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = csv_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Parquet Data\n",
    "\n",
    "Next, let's grab a `parquet` based dataset for the example. Same workflow as for the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_loader = (\n",
    "    mapper.\n",
    "    loader. # From the loader module\n",
    "    from_file(\"../data/[NYC][USA] Taxi Trips/PARQUET/taxisvis5M.parquet\") # To update with your own path\n",
    "    .with_columns(\"pickup_longitude\", \"pickup_latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = parquet_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Shapefile Data\n",
    "\n",
    "Finally, letâ€™s load a Shapefile-based dataset. Shapefiles have geometry built in, so no need to specify columns â€” UrbanMapper sorts it out for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"../data/[NYC][USA] MapPluto/Shapefile/MapPLUTO.shp\") # To update with your own path\n",
    ")\n",
    "\n",
    "gdf = shp_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Hugging Face\n",
    "This code loads the \"oscur/pluto\" dataset from Hugging Face, selects the training split, and converts the first 1,000 rows into a pandas DataFrame for efficient analysis and exploration. The resulting DataFrame can then be loaded into UrbanMapper using from_dataframe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve the dataset from Hugging Face\n",
    "dataset = load_dataset(\"oscur/pluto\")\n",
    "# Select the training split\n",
    "train_ds = dataset[\"train\"]\n",
    "# Convert the first 1000 rows to a DataFrame\n",
    "df = pd.DataFrame(train_ds[:1000])\n",
    "\n",
    "# Load the dataset using UrbanMapper\n",
    "df_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_dataframe(df) # To update with your dataframe\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = df_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Able To Preview Your Loader's instance\n",
    "\n",
    "Additionally, you can preview your loader's instance to see what columns you've specified and the file path you've loaded from. Pretty useful when you load a urban analysis shared by someone else and might want to check what columns are being used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.preview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "And thatâ€™s that! ðŸŽˆ Youâ€™ve loaded data from four different formats like a pro: `CSV`, `Parquet`, `Shapefile`, and datasets from Hugging Face. Now youâ€™re all set to play with modules like `urban_layer` or `imputer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
