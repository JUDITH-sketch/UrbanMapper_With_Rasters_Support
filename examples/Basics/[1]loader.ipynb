{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ‡ Welcome to the `loader` module!\n",
    "\n",
    "This is where your urban data journey begins. Whether youâ€™ve got `CSV`, `Parquet`, or `Shapefiles`, weâ€™ll get them loaded up and ready to explore. UrbanMapper provides two main ways to load data:\n",
    "\n",
    "1. **Manual Loading of Local Datasets**: You can load datasets available locally in various formats like CSV, Parquet, and Shapefiles. This is the default approach for working with your own data.\n",
    "2. **Integration with Hugging Face Dataset Library**: UrbanMapper also supports loading datasets from the Hugging Face library via the `from_dataframe()` method. This broadens the possibilities for integrating external data sources seamlessly.\n",
    "\n",
    "**Data source used**:\n",
    "- PLUTO data from NYC Open Data. https://www.nyc.gov/content/planning/pages/resources/datasets/mappluto-pluto-change\n",
    "- Taxi data from NYC Open Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "- **The OSCUR Hugging Face Dataset Source:**\n",
    "The [OSCUR Hugging Face organization](https://huggingface.co/oscur)\n",
    " hosts all datasets associated with [OSCUR](https://oscur.org/): Open-Source Cyberinfrastructure for Urban Computing, a research initiative focused on enabling reproducible, scalable, and accessible data-driven analysis for urban environments.\n",
    "By using the OSCUR datasets, you can skip downloading datasets from Google Drive or official links locally. These datasets are ready to use in all subsequent notebook examples without issue, making your workflow more efficient and seamless.\n",
    "\n",
    "**What youâ€™ll learn**:\n",
    "- How to kick off UrbanMapper.\n",
    "- Loading data from CSV, Parquet, and Shapefile formats.\n",
    "- Loading datasets from Hugging Face with UrbanMapper.\n",
    "\n",
    "Ready? Letâ€™s dive in! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_mapper as um\n",
    "\n",
    "# Start up UrbanMapper\n",
    "mapper = um.UrbanMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "First up, letâ€™s load a CSV file with PLUTO data. Weâ€™ll tell UrbanMapper where to find the longitude and latitude columns so it knows whatâ€™s what and can make sure those colums are well formatted prior any analysis.\n",
    "\n",
    "Note that below we employ a given csv, but you can put your own path, try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mapper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_loader \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmapper\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mloader \u001b[38;5;66;03m# From the loader module\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/[NYC][USA] MapPluto/CSV/pluto.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# To update with your own path\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwith_columns(longitude_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, latitude_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Inform your long and lat columns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m gdf \u001b[38;5;241m=\u001b[39m csv_loader\u001b[38;5;241m.\u001b[39mload() \u001b[38;5;66;03m# Load the data and create a geodataframe's instance\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# gdf stands for GeoDataFrame, like df in pandas for dataframes.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mapper' is not defined"
     ]
    }
   ],
   "source": [
    "csv_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"../data/[NYC][USA] MapPluto/CSV/pluto.csv\") # To update with your own path\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = csv_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Parquet Data\n",
    "\n",
    "Next, let's grab a `parquet` based dataset for the example. Same workflow as for the csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_loader = (\n",
    "    mapper.\n",
    "    loader. # From the loader module\n",
    "    from_file(\"../data/[NYC][USA] Taxi Trips/PARQUET/taxisvis5M.parquet\") # To update with your own path\n",
    "    .with_columns(\"pickup_longitude\", \"pickup_latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = parquet_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Shapefile Data\n",
    "\n",
    "Finally, letâ€™s load a Shapefile-based dataset. Shapefiles have geometry built in, so no need to specify columns â€” UrbanMapper sorts it out for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_file(\"../data/[NYC][USA] MapPluto/Shapefile/MapPLUTO.shp\") # To update with your own path\n",
    ")\n",
    "\n",
    "gdf = shp_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Raster Data\n",
    "\n",
    " Letâ€™s load a Raster file !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_mapper as um\n",
    "\n",
    "# Start up UrbanMapper\n",
    "mapper = um.UrbanMapper()\n",
    "rst_loader = (\n",
    "    mapper\n",
    "    .loader\n",
    "    .from_file(\"../../src/urban_mapper/modules/loader/tests/data/lower_manhattan_DEM.tif\")\n",
    ")\n",
    "\n",
    "rst = rst_loader.load()\n",
    "print(rst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from Hugging Face\n",
    "\n",
    "UrbanMapper provides two ways to load datasets from Hugging Face:\n",
    "\n",
    "1. **Using `from_dataframe()`**: This method allows you to load a dataset into a pandas DataFrame first, giving you flexibility to preprocess or explore the data before loading it into UrbanMapper.\n",
    "2. **Using `from_huggingface()`**: This method directly loads the dataset into UrbanMapper, skipping the intermediate DataFrame step for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Using `from_dataframe()`\n",
    "\n",
    "This code loads the \"oscur/pluto\" dataset from Hugging Face, selects the training split, and converts the first 1,000 rows into a pandas DataFrame for efficient analysis and exploration. The resulting DataFrame can then be loaded into UrbanMapper using `from_dataframe()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Retrieve the dataset from Hugging Face\n",
    "dataset = load_dataset(\"oscur/pluto\")\n",
    "# Select the training split\n",
    "train_ds = dataset[\"train\"]\n",
    "# Convert the first 1000 rows to a DataFrame\n",
    "df = pd.DataFrame(train_ds[:1000])\n",
    "\n",
    "# Load the dataset using UrbanMapper\n",
    "df_loader = (\n",
    "    mapper\n",
    "    .loader # From the loader module\n",
    "    .from_dataframe(df) # To update with your dataframe\n",
    "    .with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # Inform your long and lat columns\n",
    ")\n",
    "\n",
    "gdf = df_loader.load() # Load the data and create a geodataframe's instance\n",
    "\n",
    "# gdf stands for GeoDataFrame, like df in pandas for dataframes.\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using `from_huggingface()`\n",
    "\n",
    "This method directly loads the \"oscur/pluto\" dataset into UrbanMapper, skipping the intermediate DataFrame step. It's a simpler and faster way to load datasets hosted on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a full dataset directly from Hugging Face\n",
    "loader = mapper.loader.from_huggingface(\"oscur/pluto\", number_of_rows=100).with_columns(longitude_column=\"longitude\", latitude_column=\"latitude\")\n",
    "gdf = loader.load()\n",
    "gdf  # Next steps: analyze or visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Able To Preview Your Loader's instance\n",
    "\n",
    "Additionally, you can preview your loader's instance to see what columns you've specified and the file path you've loaded from. Pretty useful when you load a urban analysis shared by someone else and might want to check what columns are being used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.preview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "And thatâ€™s that! ðŸŽˆ Youâ€™ve loaded data from four different formats like a pro: `CSV`, `Parquet`, `Shapefile`, and datasets from Hugging Face. Now youâ€™re all set to play with modules like `urban_layer` or `imputer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:black; font-weight:bold;\">Loading Rasters</h1>\n",
    "\n",
    "This cell tests the integration of the new RasterLoader within the LoaderFactory. It attempts to load a raster file (with a fake path) and expects the loader to recognize the raster format and raise a NotImplementedError because the actual raster loading functionality is not yet implemented. The test confirms that the factory correctly dispatches the loading task to the RasterLoader and that the expected exception is raised, indicating the loader is properly integrated but the feature is still under development.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "source": [
    "<h1 style=\"color:black; font-weight:bold;\">Loading Rasters</h1>\n",
    "\n",
    "This cell tests the integration of the new RasterLoader within the LoaderFactory. It attempts to load a raster file (with a fake path) and expects the loader to recognize the raster format and raise a NotImplementedError because the actual raster loading functionality is not yet implemented. The test confirms that the factory correctly dispatches the loading task to the RasterLoader and that the expected exception is raised, indicating the loader is properly integrated but the feature is still under development.\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 2,
>>>>>>> 4b19c2a (feat : " example for testing load of ".tiff" format ")
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "âœ… Fichier raster manquant dÃ©tectÃ© : File 'fake.tif' does not exist.\n"
=======
      "âŒ Unexpected error: Level 'DEBUG_LOW' does not exist\n"
>>>>>>> 4b19c2a (feat : " example for testing load of ".tiff" format ")
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# Test d'intÃ©gration pour RasterLoader (support raster)\n",
    "from urban_mapper.modules.loader.loader_factory import LoaderFactory\n",
    "\n",
    "# Chemin fictif pour un raster (le fichier n'a pas besoin d'exister pour ce test)\n",
    "fake_raster_path = \"lower_manhattan_DEM.tif\"\n",
=======
    "# Integration test for RasterLoader (raster support)\n",
    "from urban_mapper.modules.loader.loader_factory import LoaderFactory\n",
    "\n",
    "# Fake path for a raster file (the file does not need to exist for this test)\n",
    "fake_raster_path = \"test_data/sample.tif\"\n",
>>>>>>> 4b19c2a (feat : " example for testing load of ".tiff" format ")
    "\n",
    "factory = LoaderFactory()\n",
    "try:\n",
    "    loader = factory.from_file(fake_raster_path)\n",
<<<<<<< HEAD
    "    loader.load()  # Doit lever NotImplementedError pour lâ€™instant\n",
    "except FileNotFoundError as e:\n",
    "    print(\"âœ… Fichier raster manquant dÃ©tectÃ© :\", e)\n",
    "except NotImplementedError as e:\n",
    "    print(\"âœ… RasterLoader reconnu, mais pas encore implÃ©mentÃ© :\", e)\n",
    "except Exception as ex:\n",
    "    print(\"âŒ Erreur inattendue :\", ex)\n",
    "else:\n",
    "    print(\"âŒ Erreur : Le RasterLoader aurait dÃ» lever NotImplementedError\")\n"
=======
    "    loader.load()  # Should raise NotImplementedError for now\n",
    "except NotImplementedError as e:\n",
    "    print(\"âœ… RasterLoader recognized, but not yet implemented:\", e)\n",
    "except Exception as ex:\n",
    "    print(\"âŒ Unexpected error:\", ex)\n",
    "else:\n",
    "    print(\"âŒ Error: RasterLoader should have raised NotImplementedError\")"
>>>>>>> 4b19c2a (feat : " example for testing load of ".tiff" format ")
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
>>>>>>> ab0a72a (feat "new example for testing .tiff loading")
   "metadata": {},
   "source": [
    "<h1 style=\"color:black; font-weight:bold;\">Loading Rasters</h1>\n",
    "\n",
    "This cell tests the integration of the new RasterLoader within the LoaderFactory. It attempts to load a raster file (with a fake path) and expects the loader to recognize the raster format and raise a NotImplementedError because the actual raster loading functionality is not yet implemented. The test confirms that the factory correctly dispatches the loading task to the RasterLoader and that the expected exception is raised, indicating the loader is properly integrated but the feature is still under development.\n"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> afa411f (feat : "new setters for raster_loader")
=======
>>>>>>> 412d2cc (feat : example)
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
>>>>>>> 4b19c2a (feat : " example for testing load of ".tiff" format ")
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
