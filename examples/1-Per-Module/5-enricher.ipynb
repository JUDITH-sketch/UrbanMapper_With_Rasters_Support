{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enricher\n",
    "\n",
    "In this notebook we‚Äôll sprinkle some extra magic onto your urban layers. Let‚Äôs add e.g. average building floors to a layer and see it sparkle!\n",
    "\n",
    "**Data source used**:\n",
    "- PLUTO data from NYC Open Data. https://www.nyc.gov/content/planning/pages/resources/datasets/mappluto-pluto-change\n",
    "\n",
    "Let‚Äôs jazz things up! üèôÔ∏è\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urban_mapper as um\n",
    "\n",
    "# Start UrbanMapper\n",
    "mapper = um.UrbanMapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data and Creating a Layer\n",
    "\n",
    "First, let‚Äôs grab some PLUTO data and set up a street intersections layer for Downtown Brooklyn.\n",
    "\n",
    "Note that:\n",
    "\n",
    "- Loader example can be seen in `examples/Basics/loader.ipynb`\n",
    "- Urban Layer example can be seen in `examples/Basics/urban_layer.ipynb`\n",
    "- Imputer example can be seen in `examples/Basics/imputer.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Note: For the documentation interactive mode, we only query 5000 records from the dataset.  Feel free to remove for a more realistic analysis.\n",
    "data = (\n",
    "    mapper\n",
    "    .loader\n",
    "    .from_huggingface(\"oscur/pluto\", number_of_rows=5000, streaming=True).with_columns(\"longitude\", \"latitude\").load()\n",
    "    # From the loader module, from the following file within the HuggingFace OSCUR datasets hub and with the `longitude` and `latitude` or only `geometry`\n",
    ")\n",
    "\n",
    "# Create urban layer\n",
    "layer = (\n",
    "    mapper\n",
    "    .urban_layer # From the urban_layer module\n",
    "    .with_type(\"streets_intersections\")  # With the type streets_intersections\n",
    "    .from_place(\"Downtown Brooklyn, New York City, USA\") # From place\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Impute your data if they contain missing values\n",
    "data = (\n",
    "    mapper\n",
    "    .imputer # From the imputer module\n",
    "    .with_type(\"SimpleGeoImputer\")  # With the type SimpleGeoImputer\n",
    "    .on_columns(longitude_column=\"longitude\", latitude_column=\"latitude\") # On the columns longitude and latitude\n",
    "    .transform(data, layer)  # All imputers require access to the urban layer in case they need to extract information from it.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching the Layer with Debug Enabled\n",
    "\n",
    "Now that we've gathered the ingredients let's enrich our urban layer. E.g with the average number of floors per intersection. We‚Äôll map the data, set up the enricher with the debug feature enabled, and apply it.\n",
    "\n",
    "Feel free for further readings to explore our Figma system workflow at: https://www.figma.com/board/0uaU4vJiwyZJSntljJDKWf/Developer-Experience-Flow-Diagram---Snippet-Code?node-id=0-1&t=mESZ52qU1D2lfzvH-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map data to the nearest layer\n",
    "# Here the point is to say which intersection of the city maps with which record(s) in your data\n",
    "# so that we can take into account when enriching.\n",
    "_, mapped_data = layer.map_nearest_layer(\n",
    "    data,\n",
    "    longitude_column=\"longitude\", latitude_column=\"latitude\",\n",
    "#   geometry_column=<geometry_column_name>\", # Replace <geometry_column_name> with the actual name of your geometry column instead of latitude and longitude columns.\n",
    "    output_column=\"nearest_intersection\", # Will create this column in the data, so that we can re-use that throughout the enriching process below.\n",
    ")\n",
    "\n",
    "# Set up and apply enricher with debug enabled\n",
    "enricher = (\n",
    "    mapper\n",
    "    .enricher # From the enricher module\n",
    "    .with_data(\n",
    "        group_by=\"nearest_intersection\", values_from=\"numfloors\"\n",
    "    ) # Reading: With data grouped by the nearest intersection, and the values from the attribute numfloors\n",
    "    .aggregate_by(\n",
    "        method=\"mean\", output_column=\"avg_floors\"\n",
    "    ) # Reading: Aggregate by using the mean and output the computation into the avg_floors new attribute of the urban layer\n",
    "    .with_debug()  # Enable debug to add DEBUG_avg_floors column which will contain the list of indices from the input data used for each enrichment\n",
    "    .build()\n",
    ")\n",
    "enriched_layer = enricher.enrich(\n",
    "    mapped_data, layer\n",
    ")  # Data to use, Urban Layer to Enrich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Enriched Layer with Debug Information\n",
    "\n",
    "Let‚Äôs take a look at the enriched layer, which now includes the `avg_floors` column and the `DEBUG_avg_floors` column with the list of indices from the input data used for each enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the enriched layer with debug information\n",
    "print(enriched_layer.layer[['avg_floors', 'DEBUG_avg_floors']].head(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be Able To Preview Your Enricher\n",
    "\n",
    "Fancy a peek at your enricher? Use `preview()` to see the setup‚Äîgreat for when you‚Äôre digging into someone else‚Äôs work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview enricher\n",
    "print(enricher.preview())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide many different datasets to the same enricher\n",
    "\n",
    "You can load many datasets and feed the enricher with a dictionary. All the provided datasets should have the same columns provided in `with_data`, `aggregate_by`, etc.\n",
    "\n",
    "The user can use the argument `data_id` to specify which dataset from the dictionary should be enrichered.\n",
    "\n",
    "The output will have an enriched layer, with the specific columns, and and additional `data_id` column that identifies the origin of that row based on the input dictionary keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "data1 = (\n",
    "    mapper\n",
    "    .loader\n",
    "    .from_huggingface(\"oscur/pluto\", number_of_rows=1000, streaming=True)\n",
    "    .with_columns(\"longitude\", \"latitude\")\n",
    "#    .with_columns(geometry_column=<geometry_column_name>\") # Replace <geometry_column_name> with the actual name of your geometry column instead of latitude and longitude columns.    \n",
    "    .load()\n",
    "    # From the loader module, from the following file and with the `longitude` and `latitude` or only `geometry`\n",
    ")\n",
    "\n",
    "# Load Parquet data\n",
    "data2 = (\n",
    "    mapper\n",
    "    .loader\n",
    "    .from_huggingface(\"oscur/taxisvis1M\", number_of_rows=1000, streaming=True) # To update with your own path\n",
    "    .with_columns(\"pickup_longitude\", \"pickup_latitude\") # Inform your long and lat columns\n",
    "#    .with_columns(geometry_column=<geometry_column_name>\") # Replace <geometry_column_name> with the actual name of your geometry column instead of latitude and longitude columns.    \n",
    "    .with_map({\"pickup_longitude\": \"longitude\", \"pickup_latitude\": \"latitude\"}) ## Routines like layer.map_nearest_layer needs datasets with the same longitude_column and latitude_column\n",
    "    .load() \n",
    ")\n",
    "\n",
    "data = {\n",
    "  \"pluto_data\": data1,\n",
    "  \"taxi_data\": data2,\n",
    "}\n",
    "\n",
    "# Create a new urban layer to the data\n",
    "layer = (\n",
    "    mapper\n",
    "    .urban_layer # From the urban_layer module\n",
    "    .with_type(\"streets_intersections\")  # With the type streets_intersections\n",
    "    .from_place(\"Downtown Brooklyn, New York City, USA\") # From place\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Map datasets to the nearest layer\n",
    "# Here the point is to say which intersection of the city maps with which record(s) in each of your datasets\n",
    "# so that we can take into account when enriching.\n",
    "_, mapped_data = layer.map_nearest_layer(\n",
    "    data,\n",
    "    longitude_column=\"longitude\", latitude_column=\"latitude\",\n",
    "#    geometry_column=<geometry_column_name>\", # Replace <geometry_column_name> with the actual name of your geometry column instead of latitude and longitude columns.\n",
    "    output_column=\"nearest_intersection\", # Will create this column in the data, so that we can re-use that throughout the enriching process below.\n",
    ")\n",
    "\n",
    "# Set up and apply enricher with debug enabled\n",
    "enricher = (\n",
    "    mapper\n",
    "    .enricher # From the enricher module\n",
    "    .with_data(\n",
    "        group_by=\"nearest_intersection\", values_from=\"numfloors\", data_id=\"pluto_data\"\n",
    "    ) # Reading: With data grouped by the nearest intersection, and the values from the attribute numfloors\n",
    "      #Both datasets should have the same group_by and values_from columns\n",
    "    .aggregate_by(\n",
    "        method=\"mean\", output_column=\"avg_floors\"\n",
    "    ) # Reading: Aggregate by using the mean and output the computation into the avg_floors new attribute of the urban layer\n",
    "    .with_debug()  # Enable debug to add DEBUG_avg_floors column which will contain the list of indices from the input data used for each enrichment\n",
    "    .build()\n",
    ")\n",
    "enriched_layer = enricher.enrich(\n",
    "    mapped_data, layer\n",
    ")  # Data to use, Urban Layer to Enrich.\n",
    "\n",
    "#present only the layer items with data_id\n",
    "layer = enriched_layer.layer[~enriched_layer.layer.data_id.isna()]\n",
    "layer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Enricher / Aggregators primitives ?\n",
    "\n",
    "Yes ! We deliver `cont_by` instead of `aggregate_by` which simply count the number of records rather than aggregating. Further is shown per future examples outside `Basics`.\n",
    "\n",
    "Wants more? Come shout that out on https://github.com/VIDA-NYU/UrbanMapper/issues/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "Smashing work! üéâ Your layer‚Äôs now enriched with average floors and includes debug information to trace back to the original data. Try visualising it next with `visualiser`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
